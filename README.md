# KVstorageBaseRaft-cpp
> notice：本项目的目的是学习Raft的原理，实现一个简单的分布式k-v存储数据库。
## 第三方依赖
- boost， 
- muduo
- protoc
**安装说明**
- protoc，本地版本为3.12.4，ubuntu22使用`sudo apt install protobuf-compiler libprotobuf-dev`安装默认就是这个版本
- boost，`sudo apt-get install libboost-dev libboost-test-dev libboost-all-dev`
- muduo,https://blog.csdn.net/QIANGWEIYUAN/article/details/89023980

## 编译
```bash
cd build/ && rm -rf *
cmake .. && make -j8
```
## 运行
```bash
cd bin/

#启动raft集群
./raftCoreRun -n 3 -f test.conf
#启动客户端
./callerMain
```

## 项目介绍

## 跳表讲解
什么是跳表?
跳表是一种概率性数据结构，基于多层有序链表，每一层都是下一层的一个子集。最底层是原始数据的完整列表，每个元素在上面一层出现的概率是一样的(一般取1/2)
举例：我们从原链表中选一些关键节点出来作为索引层，这样就形成了2层链表，即跳表。举个例子：假设有6个节点，查k=6节点要从头遍历到尾。在跳表表中则不一样了，假设我们取1，3，5三个节点作为索引层。查询先查1<6，直接跳到k=3<6,再跳到k=5<6，但到头了，跳到下一层，查找中我们跳过了k=2和4。
然后我们再保证每2层之间的的节点比是2:1，就能保证logn的查询

记住：只有最底层是真正的数据层，上面的索引层都是数据在维护。

跳表层级如何确定：随机确定
理想情况下，我们希望每隔一个节点就选一个作为上层索引，保证logn的查询
就是说我们希望每个节点都有50%的概率被选到第2层，有25%的概率被选到第3层(50%*50%)，实现这个就很简单了：rand()得到随机数，奇数加一层，偶数就停，确定层级。这种方法的原理叫大数定律，当进行足够多的实验，某个事件发生的概率会接近理论概率。

跳表节点的数据结构：key, value, 层级，前向指针(2级指针，就是一个指针数组，p[i]本结点在第i层的下一个节点地址)
跳表的搜索过程：从顶层开始比较，小于目标节点则向右遍历，大于或着遍历到头了，就下沉移动到下一层，继续查找
插入：1、从最高层开始逐层确定插入位置，每层找到一个最接近但比它小的节点，记下它的位置，因为后续可能要更新这个节点next指针。
       2、随机确定层级，若层级高于当前跳表的最高层级，那所有的更高层级，虚拟头节点就作为前驱节点；剩下就是在各层插入新节点，同时更新前节点的next指针。 至此，节点插入到跳表，且跳表的索引层也全部更新。
删除：删除与插入同理

为什么zset使用跳表不用平衡树(红黑树、AVL树)？
1. 跳表实现要更简单；
2. 跳表扩展更容易，添加层级更新指针即可，不需要重平衡操作；
3. 并发比平衡树更友好，平衡树增删节点后需要旋转进行重平衡，往往加的锁粒度都比较大，比如说全局锁。而跳表要增删节点只需要对该节点所在的那些层的相邻节点给加锁，即可。锁的粒度小。
以上是跳表的优点，而zset作为一个有序集合，典型应用场景是排行榜，对并发要求比较高。所以他选择跳表，可以更简单更高效支持并发操作。

那为什么mysql不用跳表用B+树呢？
数据量的问题：b+树优势在于低层级存大量数据，换成跳表层级太高，且跳表的每次跳转都是一次磁盘io，对于磁盘io密集型的场景，不太适合。


## raft讲解
参考视频链接：https://www.bilibili.com/video/BV1WN411c7xh/?p=4&spm_id_from=pageDriver
文档：docs/raft

CAP理论
1. 一致性：所有分布式的数据在任何时候都要保持一致。一致性可能会导致系统不可用
2. 可用性：系统中所有正常的节点收到请求后必须给出响应，保证系统对外是可用的。
3. 分区容错性：分区是指分布式系统节点间通信出现问题，我们就称出现了分区。分区容错性就是说即使出现分区，集群依旧要正常运行。
4. 一般都是3选2,而p几乎是必选，所以基本上分布式系统要么是AP模型，要么是CP模型。这2种模型的意思是在分布式系统出现问题时，你是优先保证一致性，还是优先保证可用性。如：银行流水，必须是AP， 而会员昵称可以是CP。
5. 但是要注意：并不是说保证了一致性，可用性就是完全不可用，只是需要权衡的时候你优先选择保证那一边。
6. raft算法就是要保证它的CAP，使得系统在任何情况下都能够对外提供一致的数据；且部分节点故障下，依旧能提供服务；分区恢复后，能够自动合并数据一致性。

状态机：Raft的上层应用，可是任意存储层，本项目是KV数据库
Term: 任期，是Raft集群内部的逻辑时钟；
日志：一条执行命令，类似redis的AOF日志。raft算法的⽬的：保证各节点⽇志⼀致->上层状态机执⾏的命令⼀致->上层状态机数据⼀致。

什么是raft,基本原理？
Raft算法是一种分布式算法，旨在保证分布式系统中的数据一致性。
他把系统中的节点分为三类角色：追随者（接受leader的⼼跳和⽇志同步请求，投票给候选人）和候选人（选举过程中的临时角色，参与竞选leader）、leader领导者(统领整个集群，负责处理客户端请求、更新数据到其他节点)。
Raft算法的基本原理包括以下几点：
1. 领导者选举（Leader Election）：在系统启动时或者当前领导者失效时，节点会发起选举过程。节点会在一个随机的超时时间内等待收到来自其他节点的心跳消息。如果在超时时间内没有收到心跳消息，节点就会成为候选人并发起选举。候选人向其他节点发送投票请求，并在得到大多数节点的投票后成为新的领导者。
2. 日志复制（Log Replication）：一旦领导者选举完成，新的领导者就会接收客户端的请求，并将更新的日志条目复制到其他节点。当大多数节点都成功复制了这些日志条目时，更新被认为是提交的，并且可以应用到节点的状态机中。
3. 安全性（Safety）：Raft算法保证集群最多只有一个leader(即使出现分区)、而且leader的日志条数一定比一半以上的要多或者相等。这样就保证了数据的安全性。


选举过程：
举例说明：
1. 假设现在有5个节点，开始大家都是追随者，每个节点raft都会设置一个随机超时时间，时间内未收到leader心跳就发起选举。
2. 现在节点1先超时，他就把自己的term+1,身份变为候选人，给自己投一票，并向其他节点发起RPC拉票请求。其他节点收到后发现你的term比我大，且日志条数>=我，就会把票头给他，然后把自己的term更新为s1的term。当s1收到半数以上的票，就成为leader，定时发送心跳。
3. 当节点1挂了，其他4个节点时间内未收到心跳，则又会发起选举，步骤与之前一样。但可能会出现一种情况：投票分裂：假设节点1还是宕机的，节点2和节点3同时超时，出现2个候选人，最终节点4投给节点2，节点5投给节点3，现在2，3节点都是2票，未达到半数，就会继续向节点1拉票(因为他们不知道节点1宕机了)，这样下去很肯定是没结果。一段时间后，节点4也超时了，开始拉票，而且它的term肯定是最大的，所以2，3，5三个节点都会投票给他，成为leader,同时其他人身份恢复为追随者。

问题：
  为什么要获得半数以上的票才能当leader？为了保证只有一个leader。
     如果发现一个leader，但是其term小于自己会发生什么？发起新选举，且通知过时的leader。这种情况是可能出现的：网络分区。
随机超时时间的目的？防止同一时间出现太多的候选人导致无法选出leader。
超时时间怎么设置的？
每100ms发一次心跳，每300-500ms发起一次选举


日志复制过程：
1. 领导者接收到客户端的写请求后，把请求写到自己的日志数组中，然后向所有的follower发起日志同步请求：要么发快照，要么发日志数组。
若该节点需要同步的日志已经被打进快照了就发快照给他，否则就发日志数组。并且leader有一个数组专门记录了个节点与它的同步情况，他根据
这个数组来发送相应的日志进行同步。
2. 当半数以上的节点完成同步，leader就会把这条日志commit应用到上层状态机，然后让其他节点也commit。
3. 若出现分区，可能会导致有的节点日志不匹配，如何解决？先说结论：强leader一致性。如leader发了5条日志过来，从index=5位置从后往前匹配，第5条不匹配，第4条不匹配，第3条匹配，那说明123都是匹配的(因为日志的顺序一致性)，则把4567覆盖为leader的45。

快照：
快照是某个瞬间的数据，而日志是一条条命令，通过快照来同步就是把数据直接应用到上层状态机：先要把状态机清空，然后把数据给读进去。快照的数据与raft没有任何关系，没有什么term,index
快照是怎么制作的？ 当日志数目达到一定的阈值，就打一个快照：跳表的所有数据用protobuf序列化成2进制也就是bin数据，并且落盘存着。
多久制作一个快照：每隔1000个日志条目我们生成新快照

Raft算法如何处理节点故障？
节点故障和网络分区是一个意思，都是节点间无法通信。
1. 小于半数的节点故障：若leader也挂了，集群就会重新选举，选出新leader恢复工作；若leader没挂，那集群正常对外提供服务，面对写请求，只要半数以上的通过完成同步即可。
2. 大于半数的节点故障：不管leader挂没挂，集群会对外停止写服务。因为永远获得不到半数以上的票完成日志同步。
读服务要分类：若leader挂了，读不了；若leader没挂：还是可以读的

你的分布式系统只有leader能写，如何面对高并发？
参考redis集群，对整个主从架构进行水平扩展。

Raft算法在实际场景中的应用有哪些？是否了解一些使用Raft的实际系统案例？
1. 一些常见的配置中心，为了保证可用会采用Raft，比如zookeeper的底层实现了基于Raft修改的算法，ETCD等。
2. 一些分布式数据库，比如TIKV，TiDB

Raft算法在分布式系统中有哪些常见的问题和挑战？
1. Leader节点负责所有的客户端请求的处理和日志复制，这可能会成为系统的瓶颈。（允许从追随者读）
2. 日志复制延迟：Raft算法要求日志复制必须在大多数节点上完成后才能提交，这可能导致日志复制的延迟，影响系统的实时性能。
3. 网络分区：网络分区恢复后数据的合并比较复杂。
——————————————————————————————————————————————————————————
代码：
Raft层
1. 看raft类的成员和函数声明，重点是成员变量，函数都是给他们服务的
   init函数：启动三个定时器线程分别维护：选举、日志同步和心跳、raft与上层状态机之间的联系。
  超时时间每次选300-500的随机值, 发送心跳间隔：100ms
（1）leaderHearBeatTicker()：选举定时器，raft类有一个成员：m_lastResetElectionTime(记录上一次收到心跳的时间点)，我们这个线程就定期去检查这个变量，睡眠时间就设为超时时间，醒来后若发现变量没变，则说明一个超时时间内没收到心跳，那就发起选举。若变量变化了，就继续睡。
  doElection()：term+1,给自己投一票，向其他节点发起RPC拉票请求，把自己的term,日志的index都发过去，追随者节点收到后比较一下term和日志，选择是否投票给他。若本轮时间没成为leader，就发起新一轮选举

（2）leaderHearBeatTicker:维护日志复制、和心跳(因为日志复制其实也是一个心跳)，定时向其他节点发送日志复制或心跳，日志复制这里是2条路：是发送快照还是发送日志条目，怎么选？如你要向1节点发起同步请求，但你一看1节点需要的日志已经被打进快照，那就发快照，不发日志了。若没有打进快照，就查日志同步数组，发日志给他。追随者收到后比较一下term和日志新旧，选择是否同步。
（3）applierTicker() ；定期把commit了的日志应用到上层状态机里，并根据日志数判断是否要打快照

持久化层
哪些数据需要持久化？ 
     kvdb的快照、日志数组(他俩加一起就是所有的日志了)，term, m_votedFor(投票情况)
   什么时候持久化：当要持久化的内容发生改变的时候如term等。
谁来调用持久化？谁调都行，这里就用raft自己来调


客户端：
clinet需要与leader节点建立联系：
1. 可以随意向某个节点发起请求，然后让他转发给leader
2. 把leader注册进zk。